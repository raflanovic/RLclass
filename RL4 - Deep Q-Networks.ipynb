{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class 4: Deep Q Networks.**\n",
    "\n",
    "1. <a href=\"#sec1\">Environments</a>\n",
    "    1. <a href=\"#sec1.1\">Cartpole</a>\n",
    "    2. <a href=\"#sec1.2\">Cartpole swing-up</a>\n",
    "    3. <a href=\"#sec1.3\">Pong</a>\n",
    "2. <a href=\"#sec2\">Value Iteration as a sequence of Supervized Learning problems</a>\n",
    "3. <a href=\"#sec3\">Experience Replay</a>\n",
    "4. <a href=\"#sec4\">A deep Q-network</a>\n",
    "5. <a href=\"#sec5\">Making DQN more efficient</a>\n",
    "    1. <a href=\"#sec5.1\">Target network</a>\n",
    "    2. <a href=\"#sec5.2\">Error clipping</a>\n",
    "6. <a href=\"#sec6\">DQN on image-based tasks</a>\n",
    "7. <a href=\"#sec7\">Going further</a>\n",
    "\n",
    "Let's start with this quote:\n",
    "\n",
    "> The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning. When an infant plays, waves its arms, or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection to its environment. Exercising this connection produces a wealth of information about cause and effect, about the consequences of actions, and about what to do in order to achieve goals. Throughout our lives, such interactions are undoubtedly a major source of knowledge about our environment and ourselves. Whether we are learning to drive a car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and we seek to influence what happens through our behavior. Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence. (Richard S. Sutton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=sec1></a> Environments\n",
    "\n",
    "In this session, we will work with three different environments:\n",
    "- CartPole\n",
    "- A modified version of CartPole\n",
    "- Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "gym.logger.set_level(gym.logger.DISABLED)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=sec1.1></a>CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [OpenAI Gym website](https://gym.openai.com/envs/CartPole-v0/)\n",
    "\n",
    "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n",
      "{'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}\n"
     ]
    }
   ],
   "source": [
    "print(cartpole.action_space)\n",
    "print(cartpole.observation_space)\n",
    "print(cartpole.env.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "x = cartpole.reset()\n",
    "cartpole.render()\n",
    "for i in range(1000):\n",
    "    _,_,d,_ = cartpole.step(np.random.randint(2))\n",
    "    cartpole.render()\n",
    "    if d:\n",
    "        print(i)\n",
    "        break\n",
    "cartpole.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=sec1.2></a>CartPole Swing-up\n",
    "\n",
    "The idea here is to keep the same environment, but instead of learning how to stabilize it around the unstable equilibrium point, we'd like to learn to swing it up. Here are the changes:\n",
    "- `reset` now puts the pole pointing down. The initial state is $(0,0,\\pi,0)$ plus a vector of four uniformly random values in [-0.05,0.05].\n",
    "- `step` now returns `done=True` when the cart leaves the $x\\in [-2.4,2.4]$ interval or when the pole swings faster than $4\\pi$ radians per second.\n",
    "- the reward is still +1 for keeping the pole within 12 degrees of the vertical, it is 0 for all other time steps and -10 for swinging too fast or exiting the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleSwingUp(gym.Wrapper):\n",
    "    def __init__(self, env, **kwargs):\n",
    "        super(CartPoleSwingUp, self).__init__(env, **kwargs)\n",
    "        self.theta_dot_threshold = 4*np.pi\n",
    "    def reset(self):\n",
    "        self.state = [0,0,np.pi,0] + super().reset()\n",
    "        #self.state = [0,0,np.pi,0] + self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n",
    "    def step(self, action):\n",
    "        state = self.state\n",
    "        state,reward,done,_ = super().step(action)\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        \n",
    "        done =  x < -self.x_threshold \\\n",
    "                or x > self.x_threshold \\\n",
    "                or theta_dot < -self.theta_dot_threshold \\\n",
    "                or theta_dot > self.theta_dot_threshold\n",
    "        done = bool(done)\n",
    "        \n",
    "        if done:\n",
    "            # game over\n",
    "            reward = -10.\n",
    "            if self.steps_beyond_done is None:\n",
    "                self.steps_beyond_done = 0\n",
    "            else:\n",
    "                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "                self.steps_beyond_done += 1\n",
    "        else:\n",
    "            if theta < -self.theta_threshold_radians \\\n",
    "                or theta > self.theta_threshold_radians:\n",
    "                # pole upright\n",
    "                reward = 1.\n",
    "            else:\n",
    "                # pole swinging\n",
    "                reward = 0.\n",
    "\n",
    "        return np.array(self.state), reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "swingup = CartPoleSwingUp(gym.make('CartPole-v1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "x = swingup.reset()\n",
    "swingup.render()\n",
    "for i in range(1000):\n",
    "    _,_,d,_ = swingup.step(np.random.randint(2))\n",
    "    swingup.render()\n",
    "    if d:\n",
    "        print(i)\n",
    "        break\n",
    "swingup.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=sec1.3></a>Pong\n",
    "\n",
    "Let's build an agent that learns to play Pong, one of the [Atari games](https://github.com/openai/gym/blob/master/gym/envs/atari/atari_env.py) in Gym (originally in the [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)). You might want to try different games later on (like the popular Breakout game for instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong = gym.make('Pong-v4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the environment's description.\n",
    "> Maximize your score in the Atari 2600 game Pong. In this environment, the observation is an RGB image of the screen, which is an array of shape (210, 160, 3). Each action is repeatedly performed for a duration of k frames, where k is uniformly sampled from $\\{2, 3, 4\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n",
      "(210, 160, 3)\n",
      "0\n",
      "255\n",
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "print(pong.observation_space)\n",
    "print(pong.observation_space.shape)\n",
    "print(np.min(pong.observation_space.low))\n",
    "print(np.max(pong.observation_space.high))\n",
    "print(pong.action_space)\n",
    "#help(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = pong.reset()\n",
    "plt.imshow(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:** What is the number of possible states? Why is this not an MDP? What would one need to turn this back into an MDP?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "One frame is a $210\\times 160$ RGB image with a 256 color palette, so the set of all possible frames has size $256^{210 \\times 160 \\times 3} \\sim 10^{242579}$. That's a little too many for an efficient enumeration. Of course, most of the possible images will never occur in a Breakout game and the true state space is actually a much smaller subset of the full set of possible images. Nevertheless, unless we provide a large engineering effort in describing the state space with few variables (which would be contradictory of our goal of a \"human-level\" AI) we will need to automatically discover some structure in the state sampled data.\n",
    "\n",
    "This is not an MDP because the transition dynamics do not respect Markov's property. The probability of transitioning from $s_t$ to $s_{t+1}$ is *not* independent of previous states. The problem here is that a single frame of the game does not reflect the velocity of the ball.\n",
    "\n",
    "To recover Markov's property one could simply stack a few frames together in the state space.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 18 buttons on the Atari controller. However not all games use all buttons. Our interface to Pong specifies 6 possible actions:\n",
    "- 0 NOOP (no operation)\n",
    "- 1 FIRE (press fire button, doesn't do anything in Pong)\n",
    "- 2 RIGHT (actually moves the paddle up in Pong)\n",
    "- 3 LEFT (actually moves the paddle left in Pong)\n",
    "- 4 UP (moves the paddle upwards)\n",
    "- 5 DOWN (moves the paddle downwards)\n",
    "\n",
    "It goes up to the 6th action for naming consistency (UP and DOWN), because the other actions are not really useful.\n",
    "\n",
    "Also, for an unknown reason, the game does not start until the 20th frame (but always starts automatically, pressing FIRE does not change anything).\n",
    "\n",
    "The frame rate is 60Hz.\n",
    "\n",
    "To avoid confusion between the 6 actions allowed by Gym, let's build a wrapper around our environment, with only 2 possible actions (\"0\" for UP and \"1\" for DOWN) and a downscaled observation space. Unless you're curious and want to dig in the code, you can simply run the following cells and just use the resulting environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import AtariPreprocessing\n",
    "import cv2\n",
    "\n",
    "class PongWrapper(AtariPreprocessing):\n",
    "    def __init__(self, env, **kwargs):\n",
    "        super(PongWrapper, self).__init__(env, **kwargs)\n",
    "    def step(self,action):\n",
    "        return super(PongWrapper, self).step(4+action)\n",
    "    def _get_obs(self):\n",
    "        if self.frame_skip > 1:  # more efficient in-place pooling\n",
    "            np.maximum(self.obs_buffer[0], self.obs_buffer[1], out=self.obs_buffer[0])\n",
    "        obs = cv2.resize(self.obs_buffer[0], (84, 110), interpolation=cv2.INTER_AREA)[17:101,:]\n",
    "\n",
    "        if self.scale_obs:\n",
    "            obs = np.asarray(obs, dtype=np.float32) / 255.0\n",
    "        else:\n",
    "            obs = np.asarray(obs, dtype=np.uint8)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong = PongWrapper(gym.make('PongNoFrameskip-v4'),\n",
    "                   noop_max=0,\n",
    "                   frame_skip=4,\n",
    "                   terminal_on_life_loss=True,\n",
    "                   grayscale_obs=True,\n",
    "                   scale_obs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (84, 84), min = 0.34117648, max = 0.9137255, dtype = float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trying a random agent in Pong\n",
    "import time\n",
    "\n",
    "pong.reset()\n",
    "pong.render()\n",
    "for i in range(60):\n",
    "    a = np.random.randint(2)\n",
    "    x,r,_,_=pong.step(a)\n",
    "    pong.render()\n",
    "    #print('\\r', \"reward\", r, end=\"\")\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "pong.close()\n",
    "print(\"shape: \", x.shape, \", min = \", x.min(), \", max = \", x.max(), \", dtype = \", x.dtype, sep='')\n",
    "plt.imshow(x, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=sec2></a>Value Iteration as a sequence of Supervized Learning problems\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:** start from the Value Iteration update and write $Q_{n+1}$ as a regression problem with parameters $\\theta_{n+1}$. Specify the loss function minimized for this regression problem.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "Let's start over from the beginning. We want to find:\n",
    "$$\\pi^*(s) = \\arg\\max_{\\pi} \\mathbb{E}_{\\left(r_t\\right)_{t\\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\ \\Big| \\ s, \\pi \\right], \\ \\forall s\\in S.$$\n",
    "\n",
    "We have seen this was equivalent to finding the optimal value function $Q^*$:\n",
    "$$Q^*(s,a) = \\max_{\\pi} \\mathbb{E}_{\\left(r_t\\right)_{t\\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\ \\Big| \\  s,a, \\pi \\right], \\ \\forall (s,a) \\in S\\times A.$$\n",
    "\n",
    "And we have established that $Q^*$ was the only solution to Bellman's optimality equation:\n",
    "$$Q(s,a) = \\mathbb{E}_{s' \\sim p(\\cdot|s,a)} \\left[r(s,a,s') + \\gamma \\max_{a'} Q(s',a')\\right].$$\n",
    "\n",
    "By writing $T^*$ Bellman's optimality operator, we have, by definition:\n",
    "$$(T^*Q)(s,a) = \\mathbb{E}_{s' \\sim p(\\cdot|s,a)} \\left[r(s,a,s') + \\gamma \\max_{a'} Q(s',a')\\right].$$\n",
    "\n",
    "And thus, $Q^*$ is the only solution to $Q=T^*Q$.\n",
    "\n",
    "It appears $T^*$ is a contraction mapping on the $\\mathcal{F}(S\\times A,\\mathbb{R})$ space. Value Iteration exploits this property to build the sequence $Q_{n+1} = T^*Q_n$ which converges to $Q^*$.\n",
    "\n",
    "Let's now suppose that $Q_n$ is a function approximator, whose parameters are $\\theta_n$. We shall write $Q_n(s,a) = Q(s,a;\\theta_n)$. Then building $\\theta_{n+1}$ knowing $\\theta_n$ is the regression problem that minimizes the loss:\n",
    "\\begin{gather}\n",
    "L_n(\\theta) = \\left\\| y(s,a) - Q(s,a;\\theta) \\right\\|,\\\\\n",
    "\\textrm{with } y(s,a) = \\mathbb{E}_{s' \\sim p(\\cdot|s,a)} \\left[r(s,a,s') + \\gamma \\max_{a'} Q(s',a',\\theta_{n})\\right].\n",
    "\\end{gather}\n",
    "\n",
    "If this loss can be optimized and goes to zero, then we have found the true $Q_{n+1}$. If not, then we have found an approximation of it in the norm used to define $L_n$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:** use the L2 norm in the loss of the regression problem above, then write the gradient of the loss with respect to the regressor's parameters. Use this to introduce a stochastic gradient descent method to find $\\theta_{n+1}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "Our loss becomes:\n",
    "\\begin{gather}\n",
    "L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho(\\cdot)}\\left[ \\big( y(s,a) - Q(s,a;\\theta) \\big)^2 \\right],\\\\\n",
    "\\textrm{with } y(s,a) = \\mathbb{E}_{s' \\sim p(\\cdot|s,a)} \\left[r(s,a,s') + \\gamma \\max_{a'} Q(s',a',\\theta_{n})\\right].\n",
    "\\end{gather}\n",
    "\n",
    "In the expression above, $\\rho$ is a distribution over the state-action space. Often, it is considered to be the behavior distribution, that is the distribution of samples under the current behavior policy, like $\\epsilon$-greedy. Note that this choice is debatable.\n",
    "\n",
    "So the gradient of this loss is:\n",
    "\\begin{gather}\n",
    "\\nabla_\\theta L_n(\\theta) = \\mathbb{E}_{(s,a) \\sim \\rho(\\cdot)}\\left[ \\big( y(s,a) - Q(s,a;\\theta) \\big) \\nabla_\\theta Q(s,a;\\theta) \\right]\\\\\n",
    "\\textrm{with } y(s,a) = \\mathbb{E}_{s' \\sim p(\\cdot|s,a)} \\left[r(s,a,s') + \\gamma \\max_{a'} Q(s',a',\\theta_{n})\\right].\n",
    "\\end{gather}\n",
    "\n",
    "And when we wrap all this together:\n",
    "$$\\nabla_\\theta L_n(\\theta) = \\mathbb{E}_{(s,a) \\sim \\rho(\\cdot)}\\left[ \\left( \\mathbb{E}_{s' \\sim p(\\cdot|s,a)} \\left[r(s,a,s') + \\gamma \\max_{a'} Q(s',a',\\theta_{n})\\right] - Q(s,a;\\theta) \\right) \\nabla_\\theta Q(s,a;\\theta) \\right]$$\n",
    "\n",
    "$$\\nabla_\\theta L_n(\\theta) = \\mathbb{E}_{\\substack{(s,a) \\sim \\rho(\\cdot)\\\\ s' \\sim p(\\cdot|s,a)}}\\left[ \\left( r(s,a,s') + \\gamma \\max_{a'} Q(s',a',\\theta_{n}) - Q(s,a;\\theta) \\right) \\nabla_\\theta Q(s,a;\\theta) \\right]$$\n",
    "\n",
    "Rather than computing the full expectations in the above gradient, it is often computationally expedient to optimise the loss function by stochastic gradient descent. Then the stochastic estimate of the gradient is given by:\n",
    "$$\\nabla_\\theta L_n(\\theta) \\approx d_n(\\theta) = \\sum_{i=1}^B \\left[ \\left( r_i + \\gamma \\max_{a'} Q(s_i',a',\\theta_{n}) - Q(s_i,a_i;\\theta) \\right) \\nabla_\\theta Q(s_i,a_i;\\theta) \\right],$$\n",
    "where $\\left\\{ \\left(s_i,a_i,r_i,s'_i\\right) \\right\\}_{i\\in {1,B}}$ is a mini-batch of samples drawn independently, with $(s,a) \\sim \\rho(\\cdot)$ and $s' \\sim p(\\cdot | s,a)$.\n",
    "\n",
    "The stochastic gradient descent procedure builds a sequence of parameter values $\\theta_i$ such that:\n",
    "$$\\theta_{i+1} \\leftarrow \\theta_{i} - \\alpha d_n(\\theta_{i})$$\n",
    "\n",
    "By repeating such gradient steps, one progressively minimizes $L_n(\\theta)$ and finds $\\theta_{n+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important remark:**\n",
    "\n",
    "In the update above, if the mini-batch contains a single element, the 1-sample update is precisely that of Q-learning! Then, in Q-learning, the new loss $L_{n+1}$ is defined and the process is repeated. Consequently, there is a new loss function at each time step.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:** can you spot the (subtle but essential) difference with Q-learning?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "Recall Q-learning. The update was indeed the same, except that at any time step the mini-batch's single element was not sampled independently from the previous minibatch! Indeed, $s'$ from the previous time step becomes $s$ for the current time step. So the successive mini-batches' elements are not drawn iid.\n",
    "\n",
    "That's a key difference that questions the foundation of Q-learning in itself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=sec3></a>Experience Replay\n",
    "\n",
    "To recover the independence assumption between samples, we can introduce the mechanism of *Experience Replay* by storing past samples into a *Replay Memory*. When samples a required for a mini-batch gradient update, the samples are collected uniformly from the replay memory, thus mimicking an (almost) independent draw according to $\\rho(\\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** design a class for the replay memory of the cart-pole example(s). Limit the size of this memory (via a FIFO mechanism) to $10^6$ samples (adapt this number to your computer's RAM). Test it by running a random policy for $2\\cdot 10^6$ time steps.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-fe1bbec77f0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcartpole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#     swingup.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-fe1bbec77f0c>\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, experience)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Keep the last memory_size number of experiences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, memory_size=int(1e6)):\n",
    "        \"\"\" Data structure used to hold game experiences \"\"\"\n",
    "        # Memory will contain [state,action,next_state,reward,done]\n",
    "        self.memory = []\n",
    "        self.memory_size = memory_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        \"\"\" Adds list of experiences to the memory \"\"\"\n",
    "        # Extend the stored experiences\n",
    "        self.memory.extend(experience)\n",
    "        # Keep the last memory_size number of experiences\n",
    "        self.memory = self.memory[-self.memory_size:]\n",
    "        \n",
    "    def sample(self, size):\n",
    "        \"\"\" Returns a sample of experiences from the memory \"\"\"\n",
    "        sample_idxs = np.random.randint(len(self.memory),size=size)\n",
    "        sample_output = [self.memory[idx] for idx in sample_idxs]\n",
    "        sample_output = np.reshape(sample_output,(size,-1))\n",
    "        return sample_output\n",
    "\n",
    "state = cartpole.reset()\n",
    "exp_replay = ExperienceReplay(memory_size = int(2e6))\n",
    "# swingup.render()\n",
    "for i in range(int(2e6)):\n",
    "    a = np.random.randint(2)\n",
    "    next_state, r, d, _ = cartpole.step(a)\n",
    "    exp_replay.add([[state, a, next_state, r, d]])\n",
    "    state = next_state\n",
    "#     swingup.render()\n",
    "# swingup.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=sec4></a>A deep Q-network\n",
    "\n",
    "The term Deep Q-Network was coined by the (now historical) paper **[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)** by Mnih et al. (2013) that put forward the main ideas we develop here. All those were later popularized by DeepMind's paper in Nature **[Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)** by Mnih et al. (2015).\n",
    "\n",
    "Let's design a (deep) neural network that will serve as a function approximator for $Q(s,a;\\theta)$. \n",
    "\n",
    "<img src=\"images/dqlas.png\" height=\"15%\" width=\"15%\"></img>\n",
    "\n",
    "Note that since we're going to have to compute $\\max_a Q(s,a)$ it is preferable to avoid running as many passes through the network as there are actions. Therefore, instead of the network structure above, we will prefer to use the one below.\n",
    "\n",
    "<img src=\"images/dqls.png\" height=\"30%\" width=\"30%\"></img>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** declare a neural network for our Q function. For the CartPole task, you can use a simple network with 2 hidden layers and 16 neurons on each layer. For the SwingUp task, go up to 50 neurons per layer. For Pong... well wait a minute, we'll get to it later.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNet, self).__init__()\n",
    "        self.input_layer = nn.Linear(4, 4)\n",
    "        self.hidden_1 = nn.Linear(4, 16)\n",
    "        self.hidden_2 = nn.Linear(16, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = F.relu(self.hidden_1(x))\n",
    "        x = F.relu(self.hidden_2(x))\n",
    "        x = F.relu(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost there. Now you can implement the algorithm that:\n",
    "- takes $\\epsilon$-greedy actions with respect to $Q$\n",
    "- stores samples in the replay buffer\n",
    "- at each interaction step with the environment, draws a mini-batch, computes the target values for each $(s,a)$ and takes a gradient step.\n",
    "- repeats\n",
    "\n",
    "You can take inspiration from the algorithm on page 5 of [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602).\n",
    "\n",
    "A common optimizer (instead of plain SGD) is RMSprop. But don't run this code just yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR DEEP Q LEARNING CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=sec5></a>Making DQN more efficient\n",
    "\n",
    "\n",
    "## <a id=sec5.1></a>Target network\n",
    "It appears the code above will (probably) slowly converge to $Q^*$ but this convergence might be unstable and noisy. This can be greatly improved by taking several gradient steps on a given loss function $L_n$ instead of changing the loss function at each time step.\n",
    "\n",
    "In practice, this is achieved by the introduction of a *target network* whose parameters are noted $\\theta^-$. This idea was introduced in the [Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning) paper. At any time step, the loss becomes:\n",
    "$$L(\\theta) = \\mathbb{E}_{s,a,r,s'} \\left[ \\left( r + \\gamma \\max_{a'} Q(s',a',\\theta^-) - Q(s,a;\\theta) \\right) ^2 \\right],$$\n",
    "and the target network parameters $\\theta^-$ are only updated with the Q-network parameters $\\theta_n$ every $C$ steps and are held fixed between individual updates.\n",
    "\n",
    "This process of accumulating several gradient steps into $\\theta_n$ before updating $\\theta^-$ draws our algorithm closer to a Value Iteration scheme (or a Fitted Q-Iteration scheme for that matter).\n",
    "\n",
    "Note that more recent approaches smooth out this accumulation process by defining soft updates of the form:\n",
    "$$\\theta^- \\leftarrow \\beta \\theta^- + (1-\\beta) \\theta_n.$$\n",
    "\n",
    "## <a id=sec5.2></a>Error clipping\n",
    "\n",
    "Another common pratice to stabilize learning is to clip the value of $r + \\gamma \\max_{a'} Q(s',a',\\theta^-) - Q(s,a;\\theta)$ between $-1$ and $1$. This is not such an uncommon trick, it actually amounts to using an L2 loss for values of the loss between $-1$ and $1$ and an L1 loss outside of this domain.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** modify you code above to include a target network (with $C$ in the order of $100$) and the clipping of the error term. Then run it to learn an efficient policy for CartPole (then for SwingUp).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR IMPROVED DEEP Q LEARNING CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=sec6></a>DQN on image-based tasks\n",
    "\n",
    "Now it's time to turn towards Pong. As you noted earlier, the frame information in Pong is not sufficient to define an MDP, but stacking several frames together allows to recover the Markov property.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** modify your replay buffer to store stacks of 4 black-and-white images.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR REPLAY BUFFER FOR PONG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two DQN papers ([Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) and [Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)) actually introduce two different neural network architectures.\n",
    "\n",
    "The 2013 paper uses this architecture:\n",
    "- input: $84\\times 84\\times 4$ image (the last 4 frames)\n",
    "- layer 1: Convolutions with 16 filters of size $8\\times 8$ and stride 4. The activation is a ReLU function.\n",
    "- layer 2: Convolutions with 32 filters of size $4\\times 4$ and stride 2. The activation is a ReLU function.\n",
    "- layer 3: Fully connected with 256 ReLU units\n",
    "- layer 4 (output): Fully connected with 2 linear units (one for each action's value)\n",
    "\n",
    "The 2015 paper \n",
    "- input: $84\\times 84\\times 4$ image (the last 4 frames)\n",
    "- layer 1: Convolutions with 32 filters of size $8\\times 8$ and stride 4. The activation is a ReLU function.\n",
    "- layer 2: Convolutions with 64 filters of size $4\\times 4$ and stride 2. The activation is a ReLU function.\n",
    "- layer 2: Convolutions with 64 filters of size $3\\times 3$ and stride 1. The activation is a ReLU function.\n",
    "- layer 3: Fully connected with 512 ReLU units\n",
    "- layer 4 (output): Fully connected with 2 linear units (one for each action's value)\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Create the corresponding neural network and adapt your optimization code from the previous exercice to train on Pong (you can take $C$ much larger, in the order of $10000$).\n",
    "</div>\n",
    "Caveat: unless you have a GPU and a fair amount of time ahead of you, it is recommended to run this computation on a cloud computing service (or on a dediated machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR DQN CODE FOR PONG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you an idea of the behavior of a trained agent, you can check the following videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"p88R2_3yWPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"TmPfTpjtdgg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=sec7></a>Going further\n",
    "\n",
    "A lot of contributions have built on the initial success of DQN. Among those, some are combined and discussed in the **[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)** paper. We will simply summarize their key ideas here, by decreasing order of importance (according to the paper).\n",
    "\n",
    "- N-step returns. Use samples that accumulate several returns rather than the 1-step return of TD(0).\n",
    "- Prioritized experience replay. Inspired by the model-based Prioritized Sweeping approach, bias the distribution used to sample mini-batches in order to present high residual samples to the optimizer. This accelerates the convergence in $L_\\infty$ norm.\n",
    "- Distributional value functions. Instead of estimating $\\mathbb{E}(\\sum_t r_t)$, estimate the distribution of $\\sum_t r_t$ and iterate on it.\n",
    "- NoisyNet. Instead of an $\\epsilon$-greedy exploration strategy, introduce noise in the network's parameters to drive the exploration.\n",
    "- Dueling architecture. The neural network's architecture splits $Q$ into the estimation of a value $V(s)$ and an advantage $A(s,a)$ with shared first layers.\n",
    "- Double Q-learning. Q-learning is prone to over-estimation of the true optimal Q function (especially in high variance environments). Double Q-learning aims at compensating this weakness by introducing an under-estimation mechanism based on a second Q function.\n",
    "\n",
    "Beyond these improvements, new work is published each year that lead to better understanding of the interplay between Deep Learning and RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
